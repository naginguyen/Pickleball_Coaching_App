{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a0c24eb",
   "metadata": {},
   "source": [
    "\n",
    "# Pickleball Action Recognition — YOLOv8 Pose + LSTM (End-to-End)\n",
    "\n",
    "This notebook builds a **pickleball action recognizer** using **YOLOv8-Pose** to extract body keypoints and an **LSTM** to classify actions (e.g., *CorrectServe, DriveBackHand, DriveForehand*).  \n",
    "It includes: data prep, pose extraction, sequence building, training, realtime inference, and export for deployment.\n",
    "\n",
    "> **Run locally** with internet to `pip install` dependencies. CUDA is optional but recommended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ca26c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 1) Environment ====\n",
    "# Run once (internet required). Comment out if you've already installed.\n",
    "# If on Windows + CUDA, ensure correct torch version from https://pytorch.org/get-started/locally/\n",
    "\n",
    "# %pip install ultralytics==8.2.0 opencv-python torch torchvision torchaudio numpy scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "befa126f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = cpu\n",
      "DATA_ROOT = D:\\B3-ICT\\Group Project\n",
      "CLASSES = ['Serve', 'DriveBackHand', 'DriveForehand', 'Dink']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== 2) Imports & Global Config ====\n",
    "import os, glob, math, json, time, collections\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    import torch, torch.nn as nn, torch.optim as optim\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report\n",
    "    from ultralytics import YOLO\n",
    "except Exception as e:\n",
    "    print(\"If imports fail, please run the pip install cell above.\")\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "# --- Project configuration ---\n",
    "# Adjust these for your machine:\n",
    "DATA_ROOT = r\"D:\\B3-ICT\\Group Project\"   # your dataset root containing class folders\n",
    "OUT_DIR   = \"prepared\"                     # where to save npy arrays\n",
    "POSE_MODEL = \"yolo11s-pose.pt\"   # hoặc \"yolo11s-pose.pt\", \"yolo11m-pose.pt\", ...\n",
    "\n",
    "\n",
    "# Classes (folder names). Update to match your data:\n",
    "CLASSES = [\"Serve\", \"DriveBackHand\", \"DriveForehand\", \"Dink\"]\n",
    "\n",
    "# Sequence parameters\n",
    "SEQ_LEN = 24\n",
    "STRIDE  = 6\n",
    "\n",
    "# Keypoint filtering\n",
    "MIN_KP_CONF = 0.25\n",
    "\n",
    "# Training hyperparams\n",
    "DEVICE = \"cuda\" if (hasattr(torch, 'cuda') and torch.cuda.is_available()) else \"cpu\"\n",
    "EPOCHS = 40\n",
    "BATCH  = 64\n",
    "LR     = 1e-3\n",
    "HIDDEN = 128\n",
    "LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "\n",
    "print(\"DEVICE =\", DEVICE)\n",
    "print(\"DATA_ROOT =\", DATA_ROOT)\n",
    "print(\"CLASSES =\", CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97039dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found videos: 533\n",
      "0 D:\\B3-ICT\\Group Project\\Serve\\0805(2)-1.mp4\n",
      "0 D:\\B3-ICT\\Group Project\\Serve\\0805(2)-10.mp4\n",
      "0 D:\\B3-ICT\\Group Project\\Serve\\0805(2)-11.mp4\n",
      "0 D:\\B3-ICT\\Group Project\\Serve\\0805(2)-12.mp4\n",
      "0 D:\\B3-ICT\\Group Project\\Serve\\0805(2)-13.mp4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== 3) Utility: list videos per class ====\n",
    "\n",
    "def list_videos(root, classes):\n",
    "    vids, labels = [], []\n",
    "    for ci, cname in enumerate(classes):\n",
    "        class_dir = Path(root) / cname\n",
    "        for p in class_dir.glob(\"*.mp4\"):\n",
    "            vids.append(str(p))\n",
    "            labels.append(ci)\n",
    "    return vids, labels\n",
    "\n",
    "vids, labels = list_videos(DATA_ROOT, CLASSES)\n",
    "print(\"Found videos:\", len(vids))\n",
    "for v, y in list(zip(vids, labels))[:5]:\n",
    "    print(y, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e41ba271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 4) Pose extraction helpers ====\n",
    "\n",
    "\n",
    "L_SHOULDER, R_SHOULDER = 5, 6\n",
    "L_HIP, R_HIP = 11, 12\n",
    "\n",
    "def center_and_scale(kpts, min_scale=1e-3):\n",
    "\n",
    "    xy  = kpts[:, :2].astype(np.float32)  # (17,2)\n",
    "    conf= kpts[:, 2]\n",
    "    hips = xy[[L_HIP, R_HIP]]\n",
    "    center = hips.mean(axis=0)\n",
    "    # scale from shoulders; fallback to hips\n",
    "    if conf[L_SHOULDER] > 0 and conf[R_SHOULDER] > 0:\n",
    "        scale = np.linalg.norm(xy[L_SHOULDER] - xy[R_SHOULDER])\n",
    "    else:\n",
    "        scale = np.linalg.norm(xy[L_HIP] - xy[R_HIP])\n",
    "    scale = max(float(scale), min_scale)\n",
    "    xy_norm = (xy - center) / scale\n",
    "    return xy_norm\n",
    "\n",
    "def extract_keypoints_from_frame(results, min_conf=MIN_KP_CONF):\n",
    "    \"\"\"Select the largest person and return (17,3) [x,y,conf] or None.\"\"\"\n",
    "    if len(results) == 0:\n",
    "        return None\n",
    "    r = results[0]\n",
    "    if r.keypoints is None or r.keypoints.xy is None:\n",
    "        return None\n",
    "    xy = r.keypoints.xy\n",
    "    conf = r.keypoints.conf\n",
    "    if xy is None or conf is None or len(xy) == 0:\n",
    "        return None\n",
    "\n",
    "    # choose the largest bbox by area\n",
    "    boxes = r.boxes.xyxy.cpu().numpy() if r.boxes is not None else None\n",
    "    idx = 0\n",
    "    if boxes is not None and len(boxes) > 0:\n",
    "        areas = (boxes[:,2]-boxes[:,0])*(boxes[:,3]-boxes[:,1])\n",
    "        idx = int(np.argmax(areas))\n",
    "\n",
    "    kxy = xy[idx].cpu().numpy()                 # (17,2)\n",
    "    kcf = conf[idx].cpu().numpy().reshape(-1,1) # (17,1)\n",
    "    kpts = np.concatenate([kxy, kcf], axis=1)   # (17,3)\n",
    "\n",
    "    # too many low-confidence points -> skip\n",
    "    if (kpts[:,2] < min_conf).mean() > 0.5:\n",
    "        return None\n",
    "    return kpts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e0710c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: (2735, 24, 68) (2735,)  to prepared\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== 5) Video → per-frame features → sequences ====\n",
    "def video_to_features(path, yolo, seq_len=SEQ_LEN, stride=STRIDE):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Failed to open:\", path)\n",
    "        return [], []\n",
    "\n",
    "    frames_feats = []\n",
    "    prev_xy = None\n",
    "\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok: break\n",
    "        results = yolo.predict(frame, verbose=False)\n",
    "        kpts = extract_keypoints_from_frame(results)\n",
    "        if kpts is None:\n",
    "            frames_feats.append(None)\n",
    "            prev_xy = None\n",
    "            continue\n",
    "\n",
    "        xy_norm = center_and_scale(kpts)       \n",
    "        feat_xy = xy_norm.flatten()            \n",
    "\n",
    "        if prev_xy is not None:\n",
    "            vel = (xy_norm - prev_xy).flatten() \n",
    "        else:\n",
    "            vel = np.zeros_like(feat_xy)\n",
    "        prev_xy = xy_norm.copy()\n",
    "\n",
    "        feat = np.concatenate([feat_xy, vel], axis=0)  \n",
    "        frames_feats.append(feat)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    feats = np.array([f for f in frames_feats if f is not None])\n",
    "    if len(feats) < seq_len:\n",
    "        return [], []\n",
    "    X, idxs = [], []\n",
    "    for start in range(0, len(feats)-seq_len+1, stride):\n",
    "        X.append(feats[start:start+seq_len])  \n",
    "        idxs.append(start)\n",
    "    return X, idxs\n",
    "\n",
    "def build_dataset(data_root=DATA_ROOT, out_dir=OUT_DIR, pose_model=POSE_MODEL, classes=CLASSES):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    yolo = YOLO(pose_model)\n",
    "\n",
    "    vids, labels = list_videos(data_root, classes)\n",
    "    X_all, y_all = [], []\n",
    "    for p, y in zip(vids, labels):\n",
    "        X, _ = video_to_features(p, yolo)\n",
    "        for seq in X:\n",
    "            X_all.append(seq)\n",
    "            y_all.append(y)\n",
    "    X_all = np.array(X_all, dtype=np.float32)  # (N,T,68)\n",
    "    y_all = np.array(y_all, dtype=np.int64)\n",
    "    np.save(Path(out_dir) / \"X.npy\", X_all)\n",
    "    np.save(Path(out_dir) / \"y.npy\", y_all)\n",
    "    print(\"Saved:\", X_all.shape, y_all.shape, \" to\", out_dir)\n",
    "\n",
    "\n",
    "build_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7134f668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_acc 0.859 | val_acc 0.932 | val_loss 0.224\n",
      "Epoch 02 | train_acc 0.961 | val_acc 0.954 | val_loss 0.119\n",
      "Epoch 03 | train_acc 0.983 | val_acc 0.973 | val_loss 0.089\n",
      "Epoch 04 | train_acc 0.986 | val_acc 0.980 | val_loss 0.064\n",
      "Epoch 05 | train_acc 0.992 | val_acc 0.980 | val_loss 0.060\n",
      "Epoch 06 | train_acc 0.996 | val_acc 0.985 | val_loss 0.039\n",
      "Epoch 07 | train_acc 0.994 | val_acc 0.976 | val_loss 0.090\n",
      "Epoch 08 | train_acc 0.991 | val_acc 0.969 | val_loss 0.088\n",
      "Epoch 09 | train_acc 0.995 | val_acc 0.989 | val_loss 0.038\n",
      "Epoch 10 | train_acc 0.999 | val_acc 0.989 | val_loss 0.027\n",
      "Epoch 11 | train_acc 0.994 | val_acc 0.973 | val_loss 0.088\n",
      "Epoch 12 | train_acc 0.995 | val_acc 0.991 | val_loss 0.020\n",
      "Epoch 13 | train_acc 1.000 | val_acc 0.993 | val_loss 0.021\n",
      "Epoch 14 | train_acc 0.998 | val_acc 0.989 | val_loss 0.033\n",
      "Epoch 15 | train_acc 0.997 | val_acc 0.987 | val_loss 0.036\n",
      "Epoch 16 | train_acc 0.997 | val_acc 0.984 | val_loss 0.053\n",
      "Epoch 17 | train_acc 0.997 | val_acc 0.973 | val_loss 0.073\n",
      "Epoch 18 | train_acc 0.995 | val_acc 0.991 | val_loss 0.037\n",
      "Epoch 19 | train_acc 0.997 | val_acc 0.982 | val_loss 0.060\n",
      "Epoch 20 | train_acc 0.997 | val_acc 0.987 | val_loss 0.053\n",
      "Epoch 21 | train_acc 0.999 | val_acc 0.995 | val_loss 0.030\n",
      "Epoch 22 | train_acc 1.000 | val_acc 0.996 | val_loss 0.024\n",
      "Epoch 23 | train_acc 1.000 | val_acc 0.996 | val_loss 0.024\n",
      "Epoch 24 | train_acc 1.000 | val_acc 0.996 | val_loss 0.025\n",
      "Epoch 25 | train_acc 1.000 | val_acc 0.996 | val_loss 0.025\n",
      "Epoch 26 | train_acc 1.000 | val_acc 0.996 | val_loss 0.025\n",
      "Epoch 27 | train_acc 1.000 | val_acc 0.996 | val_loss 0.025\n",
      "Epoch 28 | train_acc 1.000 | val_acc 0.996 | val_loss 0.026\n",
      "Epoch 29 | train_acc 1.000 | val_acc 0.996 | val_loss 0.026\n",
      "Epoch 30 | train_acc 1.000 | val_acc 0.996 | val_loss 0.026\n",
      "Epoch 31 | train_acc 1.000 | val_acc 0.996 | val_loss 0.026\n",
      "Epoch 32 | train_acc 1.000 | val_acc 0.996 | val_loss 0.027\n",
      "Epoch 33 | train_acc 1.000 | val_acc 0.996 | val_loss 0.027\n",
      "Epoch 34 | train_acc 1.000 | val_acc 0.996 | val_loss 0.027\n",
      "Epoch 35 | train_acc 1.000 | val_acc 0.996 | val_loss 0.027\n",
      "Epoch 36 | train_acc 1.000 | val_acc 0.996 | val_loss 0.027\n",
      "Epoch 37 | train_acc 1.000 | val_acc 0.996 | val_loss 0.027\n",
      "Epoch 38 | train_acc 1.000 | val_acc 0.996 | val_loss 0.027\n",
      "Epoch 39 | train_acc 1.000 | val_acc 0.996 | val_loss 0.027\n",
      "Epoch 40 | train_acc 1.000 | val_acc 0.996 | val_loss 0.027\n",
      "\n",
      "Validation report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        Serve       1.00      1.00      1.00       365\n",
      "DriveBackHand       1.00      0.92      0.96        12\n",
      "DriveForehand       1.00      0.99      0.99        73\n",
      "         Dink       0.99      1.00      0.99        97\n",
      "\n",
      "     accuracy                           1.00       547\n",
      "    macro avg       1.00      0.98      0.99       547\n",
      " weighted avg       1.00      1.00      1.00       547\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== 6) LSTM model & training ====\n",
    "class ActionLSTM(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=128, layers=2, num_classes=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(in_dim, hidden, num_layers=layers, batch_first=True, dropout=dropout)\n",
    "        self.head = nn.Sequential(nn.LayerNorm(hidden), nn.Linear(hidden, num_classes))\n",
    "    def forward(self, x):               # x: (B,T,F)\n",
    "        out, _ = self.lstm(x)           # (B,T,H)\n",
    "        out = out[:, -1, :]\n",
    "        return self.head(out)           # (B,C)\n",
    "\n",
    "def batch_iter(X, y, batch=64, shuffle=True):\n",
    "    idx = np.arange(len(X))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    for i in range(0, len(X), batch):\n",
    "        sel = idx[i:i+batch]\n",
    "        yield torch.from_numpy(X[sel]).float(), torch.from_numpy(y[sel]).long()\n",
    "\n",
    "def train_lstm(x_path=Path(OUT_DIR)/\"X.npy\", y_path=Path(OUT_DIR)/\"y.npy\",\n",
    "               epochs=EPOCHS, batch=BATCH, lr=LR, hidden=HIDDEN, layers=LAYERS, dropout=DROPOUT):\n",
    "    X = np.load(x_path)   # (N,T,68)\n",
    "    y = np.load(y_path)   # (N,)\n",
    "\n",
    "    Xtr, Xva, ytr, yva = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    model = ActionLSTM(in_dim=X.shape[-1], hidden=hidden, layers=layers,\n",
    "                       num_classes=len(CLASSES), dropout=dropout).to(DEVICE)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    opt  = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    best_acc, best_path = 0.0, \"lstm_best.pt\"\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        tot, correct, loss_sum = 0, 0, 0.0\n",
    "        for xb, yb in batch_iter(Xtr, ytr, batch=batch, shuffle=True):\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            loss_sum += float(loss)\n",
    "            pred = logits.argmax(1)\n",
    "            tot += len(yb); correct += int((pred==yb).sum())\n",
    "        train_acc = correct/tot if tot>0 else 0.0\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            xb = torch.from_numpy(Xva).float().to(DEVICE)\n",
    "            yb = torch.from_numpy(yva).long().to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            va_loss = crit(logits, yb).item()\n",
    "            va_acc  = (logits.argmax(1)==yb).float().mean().item()\n",
    "\n",
    "        print(f\"Epoch {ep:02d} | train_acc {train_acc:.3f} | val_acc {va_acc:.3f} | val_loss {va_loss:.3f}\")\n",
    "        if va_acc > best_acc:\n",
    "            best_acc = va_acc\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"in_dim\": X.shape[-1],\n",
    "                \"hidden\": hidden,\n",
    "                \"layers\": layers,\n",
    "                \"num_classes\": len(CLASSES)\n",
    "            }, best_path)\n",
    "\n",
    "    # final report\n",
    "    ckpt = torch.load(best_path, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model\"]); model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(torch.from_numpy(Xva).float().to(DEVICE)).argmax(1).cpu().numpy()\n",
    "    print(\"\\nValidation report:\\n\", classification_report(yva, y_pred, target_names=CLASSES))\n",
    "\n",
    "# Uncomment to train after dataset is built:\n",
    "train_lstm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c9b9f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 7) Realtime (webcam) or video-file inference ====\n",
    "def load_lstm_checkpoint(path=\"lstm_best.pt\"):\n",
    "    ckpt = torch.load(path, map_location=DEVICE)\n",
    "    model = ActionLSTM(ckpt[\"in_dim\"], ckpt[\"hidden\"], ckpt[\"layers\"], ckpt[\"num_classes\"]).to(DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model\"]); model.eval()\n",
    "    return model\n",
    "\n",
    "def infer_webcam(pose_model=POSE_MODEL, lstm_path=\"lstm_best.pt\", seq_len=SEQ_LEN, stride=STRIDE, thresh=0.60):\n",
    "    pose = YOLO(pose_model)\n",
    "    model = load_lstm_checkpoint(lstm_path)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    buf = collections.deque(maxlen=seq_len)\n",
    "    frame_idx, last_pred, smooth = 0, None, None\n",
    "\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok: break\n",
    "        results = pose.predict(frame, verbose=False)\n",
    "        kpts = extract_keypoints_from_frame(results)\n",
    "        if kpts is not None:\n",
    "            xy_norm = center_and_scale(kpts).flatten()\n",
    "            if len(buf) > 0:\n",
    "                prev = (np.array(buf[-1])[:34]).reshape(17,2)\n",
    "                vel = (xy_norm.reshape(17,2) - prev).flatten()\n",
    "            else:\n",
    "                vel = np.zeros_like(xy_norm)\n",
    "            feat = np.concatenate([xy_norm, vel], axis=0)\n",
    "            buf.append(feat)\n",
    "\n",
    "        frame_idx += 1\n",
    "        if len(buf) == seq_len and frame_idx % stride == 0:\n",
    "            x = torch.from_numpy(np.expand_dims(np.stack(buf, axis=0), 0)).float().to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                logits = model(x)\n",
    "                probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "            smooth = probs if smooth is None else 0.6*smooth + 0.4*probs\n",
    "            cls_id = int(np.argmax(smooth))\n",
    "            conf = float(smooth[cls_id])\n",
    "            if conf >= thresh:\n",
    "                last_pred = (CLASSES[cls_id], conf)\n",
    "\n",
    "        # draw overlay\n",
    "        if last_pred:\n",
    "            text = f\"{last_pred[0]}: {last_pred[1]:.2f}\"\n",
    "            cv2.putText(frame, text, (24,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "\n",
    "        cv2.imshow(\"Pickleball LSTM+YOLOv8\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27: break\n",
    "\n",
    "    cap.release(); cv2.destroyAllWindows()\n",
    "\n",
    "def infer_video_file(video_path, pose_model=POSE_MODEL, lstm_path=\"lstm_best.pt\",\n",
    "                     seq_len=SEQ_LEN, stride=STRIDE, thresh=0.60, display=True, save_path=None):\n",
    "    pose = YOLO(pose_model)\n",
    "    model = load_lstm_checkpoint(lstm_path)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Cannot open\", video_path); return\n",
    "\n",
    "    buf = collections.deque(maxlen=seq_len)\n",
    "    frame_idx, last_pred, smooth = 0, None, None\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\") if save_path else None\n",
    "    out = None\n",
    "    if save_path:\n",
    "        w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "        out = cv2.VideoWriter(save_path, fourcc, fps, (w, h))\n",
    "\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok: break\n",
    "        results = pose.predict(frame, verbose=False)\n",
    "        kpts = extract_keypoints_from_frame(results)\n",
    "        if kpts is not None:\n",
    "            xy_norm = center_and_scale(kpts).flatten()\n",
    "            if len(buf) > 0:\n",
    "                prev = (np.array(buf[-1])[:34]).reshape(17,2)\n",
    "                vel = (xy_norm.reshape(17,2) - prev).flatten()\n",
    "            else:\n",
    "                vel = np.zeros_like(xy_norm)\n",
    "            feat = np.concatenate([xy_norm, vel], axis=0)\n",
    "            buf.append(feat)\n",
    "\n",
    "        frame_idx += 1\n",
    "        if len(buf) == seq_len and frame_idx % stride == 0:\n",
    "            x = torch.from_numpy(np.expand_dims(np.stack(buf, axis=0), 0)).float().to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                logits = model(x)\n",
    "                probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "            smooth = probs if smooth is None else 0.6*smooth + 0.4*probs\n",
    "            cls_id = int(np.argmax(smooth))\n",
    "            conf = float(smooth[cls_id])\n",
    "            if conf >= thresh:\n",
    "                last_pred = (CLASSES[cls_id], conf)\n",
    "\n",
    "        if last_pred:\n",
    "            text = f\"{last_pred[0]}: {last_pred[1]:.2f}\"\n",
    "            cv2.putText(frame, text, (24,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "\n",
    "        if display:\n",
    "            cv2.imshow(\"Pickleball LSTM+YOLOv8\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == 27: break\n",
    "        if out:\n",
    "            out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    if out: out.release()\n",
    "    if display:\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage (uncomment to run):infer_webcam()\n",
    "infer_video_file(\n",
    "    r\"D:\\B3-ICT\\Group Project\\Untitled video - Made with Clipchamp.mp4\",\n",
    "    save_path=\"hihihi.mp4\",\n",
    "    display=False        # <— turn off imshow\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2f1793b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TorchScript to lstm_ts.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_25332\\1880913183.py:16: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:4244: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OnnxExporterError",
     "evalue": "Module onnx is not installed!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\onnx\\_internal\\onnx_proto_utils.py:182\u001b[39m, in \u001b[36m_add_onnxscript_fn\u001b[39m\u001b[34m(model_bytes, custom_opsets)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01monnx\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'onnx'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOnnxExporterError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Example:\u001b[39;00m\n\u001b[32m     28\u001b[39m export_torchscript()\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mexport_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mexport_onnx\u001b[39m\u001b[34m(ckpt_path, out_path, seq_len, feat_dim)\u001b[39m\n\u001b[32m     14\u001b[39m model.load_state_dict(ckpt[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m]); model.eval()\n\u001b[32m     15\u001b[39m x = torch.randn(\u001b[32m1\u001b[39m, seq_len, feat_dim, requires_grad=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43monnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m13\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlogits\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlogits\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaved ONNX to\u001b[39m\u001b[33m\"\u001b[39m, out_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\onnx\\__init__.py:424\u001b[39m, in \u001b[36mexport\u001b[39m\u001b[34m(model, args, f, kwargs, export_params, verbose, input_names, output_names, opset_version, dynamic_axes, keep_initializers_as_inputs, dynamo, external_data, dynamic_shapes, custom_translation_table, report, optimize, verify, profile, dump_exported_program, artifacts_dir, fallback, training, operator_export_type, do_constant_folding, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dynamic_shapes:\n\u001b[32m    419\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    420\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe exporter only supports dynamic shapes \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    421\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthrough parameter dynamic_axes when dynamo=False.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    422\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m \u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\onnx\\utils.py:522\u001b[39m, in \u001b[36mexport\u001b[39m\u001b[34m(model, args, f, kwargs, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[39m\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    520\u001b[39m     args = args + (kwargs,)\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\onnx\\utils.py:1530\u001b[39m, in \u001b[36m_export\u001b[39m\u001b[34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[39m\n\u001b[32m   1511\u001b[39m     (\n\u001b[32m   1512\u001b[39m         proto,\n\u001b[32m   1513\u001b[39m         export_map,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1527\u001b[39m         node_attr_to_name,\n\u001b[32m   1528\u001b[39m     )\n\u001b[32m   1529\u001b[39m \u001b[38;5;66;03m# insert function_proto into model_proto.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1530\u001b[39m proto = \u001b[43monnx_proto_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_add_onnxscript_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1531\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1533\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m   1535\u001b[39m     _C._jit_onnx_log(\u001b[33m\"\u001b[39m\u001b[33mExported graph: \u001b[39m\u001b[33m\"\u001b[39m, graph)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\onnx\\_internal\\onnx_proto_utils.py:184\u001b[39m, in \u001b[36m_add_onnxscript_fn\u001b[39m\u001b[34m(model_bytes, custom_opsets)\u001b[39m\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01monnx\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m errors.OnnxExporterError(\u001b[33m\"\u001b[39m\u001b[33mModule onnx is not installed!\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# For > 2GB model, onnx.load_fromstring would fail. However, because\u001b[39;00m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# in _export_onnx, the tensors should be saved separately if the proto\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# size > 2GB, and if it for some reason did not, the model would fail on\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# serialization anyway in terms of the protobuf limitation. So we don't\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# need to worry about > 2GB model getting here.\u001b[39;00m\n\u001b[32m    191\u001b[39m model_proto = onnx.load_model_from_string(model_bytes)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[31mOnnxExporterError\u001b[39m: Module onnx is not installed!"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== 8) Export (TorchScript / ONNX) ====\n",
    "def export_torchscript(ckpt_path=\"lstm_best.pt\", out_path=\"lstm_ts.pt\", seq_len=SEQ_LEN, feat_dim=68):\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    model = ActionLSTM(ckpt[\"in_dim\"], ckpt[\"hidden\"], ckpt[\"layers\"], ckpt[\"num_classes\"]).cpu()\n",
    "    model.load_state_dict(ckpt[\"model\"]); model.eval()\n",
    "    example = torch.randn(1, seq_len, feat_dim)\n",
    "    traced = torch.jit.trace(model, example)\n",
    "    traced.save(out_path)\n",
    "    print(\"Saved TorchScript to\", out_path)\n",
    "\n",
    "def export_onnx(ckpt_path=\"lstm_best.pt\", out_path=\"lstm.onnx\", seq_len=SEQ_LEN, feat_dim=68):\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    model = ActionLSTM(ckpt[\"in_dim\"], ckpt[\"hidden\"], ckpt[\"layers\"], ckpt[\"num_classes\"]).cpu()\n",
    "    model.load_state_dict(ckpt[\"model\"]); model.eval()\n",
    "    x = torch.randn(1, seq_len, feat_dim, requires_grad=False)\n",
    "    torch.onnx.export(\n",
    "        model, x, out_path,\n",
    "        export_params=True,\n",
    "        opset_version=13,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['logits'],\n",
    "        dynamic_axes={'input': {0: 'batch', 1: 'time'}, 'logits': {0: 'batch'}}\n",
    "    )\n",
    "    print(\"Saved ONNX to\", out_path)\n",
    "\n",
    "# Example:\n",
    "export_torchscript()\n",
    "export_onnx()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4575fb2c",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Tips for Accuracy & Robustness\n",
    "\n",
    "- **More classes**: Dink, Volley, Smash, Drop, Lob.\n",
    "- **Ball detector**: train a tiny YOLO just for the ball and append ball (x, y, speed) to features.\n",
    "- **Better normalization**: rotate skeleton to torso-aligned coordinates.\n",
    "- **Augmentations**: horizontal flip (if class semantics allow), time-warping, subsequence jitter.\n",
    "- **Class imbalance**: oversample, use class weights.\n",
    "- **Post-processing**: hysteresis—require K consecutive steps to start/stop an event.\n",
    "- **Data splits**: split by player/session to prevent leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaae640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 10) Quickstart (edit paths, then run in order) ====\n",
    "# 1) Set DATA_ROOT and CLASSES above to match your folders.\n",
    "# 2) Run: build_dataset()\n",
    "# 3) Run: train_lstm()\n",
    "# 4) Run: infer_webcam()  or  infer_video_file(\"your_clip.mp4\", save_path=\"annotated.mp4\")\n",
    "# 5) Export: export_torchscript() or export_onnx()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
