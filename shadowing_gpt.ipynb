{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d0328b9",
   "metadata": {},
   "source": [
    "\n",
    "# üìò Shadowing Pose Trainer & Runtime (OpenCV + MediaPipe)\n",
    "**M·ª•c ti√™u**: \n",
    "1) Tr√≠ch xu·∫•t keypoints t·ª´ video (ho·∫∑c webcam) v√† **hu·∫•n luy·ªán** chu·ªói **c√°c pha A‚ÜíB‚ÜíC‚ÜíD‚ÜíE** cho m·ªói ƒë·ªông t√°c trong dataset.  \n",
    "2) **Suy lu·∫≠n real-time**: hi·ªÉn th·ªã khung x∆∞∆°ng (skeleton), t√¥ **xanh** (ƒë√∫ng) / **ƒë·ªè** (sai), k√®m c√°c **ƒëi·ªÉm A‚ÄìE** ƒë·ªÉ ng∆∞·ªùi d√πng **ch·∫°m/click** theo **tr√¨nh t·ª±** (shadowing).  \n",
    "3) X√°c ƒë·ªãnh **ƒëi·ªÉm b·∫Øt ƒë·∫ßu** / **k·∫øt th√∫c** c·ªßa m·ªôt **chu·ªói ƒë·ªông t√°c** (v√≠ d·ª• *serve*) v√† **xu·∫•t JSON** ƒë·ªÉ d√πng l·∫°i.\n",
    "\n",
    "> üí° Notebook n√†y ƒë√£ **chia nh·ªè theo cell**, k√®m **ch√∫ th√≠ch chi ti·∫øt**, d·ªÖ tu·ª≥ bi·∫øn v√† b·∫£o tr√¨.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf23f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# 1) C√ÄI ƒê·∫∂T TH∆Ø VI·ªÜN (n·∫øu c·∫ßn)\n",
    "# ==============================\n",
    "# üëâ CH·∫†Y cell n√†y tr√™n m√°y b·∫°n n·∫øu thi·∫øu th∆∞ vi·ªán.\n",
    "# L∆∞u √Ω: Google Colab / local m√°y c√≥ Internet th√¨ m·ªõi pip install ƒë∆∞·ª£c.\n",
    "\n",
    "# !pip install --upgrade pip\n",
    "# !pip install opencv-python mediapipe numpy scikit-learn pyyaml\n",
    "# Tu·ª≥ ch·ªçn: d√πng hmmlearn cho HMM (n·∫øu mu·ªën)\n",
    "# !pip install hmmlearn\n",
    "\n",
    "#print(\"N·∫øu ƒë√£ c√†i ƒë·ªß, b·∫°n c√≥ th·ªÉ b·ªè qua cell n√†y.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "960fb6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hmmlearn kh√¥ng c√≥ s·∫µn. S·∫Ω d√πng KMeans thay th·∫ø ƒë·ªÉ ph√¢n pha A‚ÜíE.\n",
      "C·∫•u h√¨nh xong. DATA_DIR: E:\\Study\\usth\\group_project\\model\\pose_videos\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================\n",
    "# 2) IMPORTS & C·∫§U H√åNH CHUNG\n",
    "# ==============================\n",
    "import os, sys, json, time, math, yaml\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Th∆∞ vi·ªán keypoints\n",
    "try:\n",
    "    import mediapipe as mp\n",
    "except Exception as e:\n",
    "    mp = None\n",
    "    print(\"mediapipe ch∆∞a ƒë∆∞·ª£c c√†i. H√£y pip install mediapipe tr∆∞·ªõc khi ch·∫°y tr√≠ch xu·∫•t/real-time.\")\n",
    "\n",
    "# H·ªçc m√°y\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# N·∫øu c√≥ hmmlearn th√¨ d√πng HMM; n·∫øu kh√¥ng, pipeline s·∫Ω fallback sang KMeans\n",
    "try:\n",
    "    from hmmlearn.hmm import GaussianHMM\n",
    "    HAVE_HMM = True\n",
    "except Exception:\n",
    "    HAVE_HMM = False\n",
    "    print(\"hmmlearn kh√¥ng c√≥ s·∫µn. S·∫Ω d√πng KMeans thay th·∫ø ƒë·ªÉ ph√¢n pha A‚ÜíE.\")\n",
    "\n",
    "# ---- C·∫•u h√¨nh ----\n",
    "ROOT = Path.cwd()               # Th∆∞ m·ª•c l√†m vi·ªác\n",
    "DATA_DIR = ROOT / r\"E:\\Study\\usth\\group_project\\model\\pose_videos\" # /<action>/<video files>\n",
    "FEAT_DIR = ROOT / \"features\"    # N∆°i l∆∞u .npz keypoints & features\n",
    "MODEL_DIR = ROOT / \"models\"     # N∆°i l∆∞u template A-E per action (JSON)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(FEAT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# DS h√†nh ƒë·ªông. V√≠ d·ª•: [\"serve\", \"forehand\", \"backhand\"]\n",
    "ACTIONS = [\"serve\", \"driveforehand\", \"drivebackhand\"]\n",
    "\n",
    "# S·ªë pha chu·∫©n ho√° cho m·ªói ƒë·ªông t√°c\n",
    "NUM_PHASES = 5  # A,B,C,D,E\n",
    "\n",
    "# Khung x∆∞∆°ng pose c·ªßa MediaPipe (33 landmarks)\n",
    "MP_LANDMARK_NUM = 33\n",
    "\n",
    "# C·∫∑p x∆∞∆°ng ƒë·ªÉ v·∫Ω (m·ªôt s·ªë c·∫∑p ti√™u bi·ªÉu)\n",
    "SKELETON_EDGES = [\n",
    "    (11,13),(13,15),  # tay tr√°i\n",
    "    (12,14),(14,16),  # tay ph·∫£i\n",
    "    (11,12),          # vai\n",
    "    (23,24),          # h√¥ng\n",
    "    (11,23),(12,24),  # th√¢n\n",
    "    (23,25),(25,27),  # ch√¢n tr√°i\n",
    "    (24,26),(26,28)   # ch√¢n ph·∫£i\n",
    "]\n",
    "\n",
    "# Ng∆∞·ª°ng ƒë√∫ng/sai m·∫∑c ƒë·ªãnh (cosine/chu·∫©n ho√°) ‚Äì s·∫Ω hi·ªáu ch·ªânh theo action sau khi train\n",
    "DEFAULT_TOL = 0.15\n",
    "\n",
    "print(\"C·∫•u h√¨nh xong. DATA_DIR:\", DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1fc9b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded math utils.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =====================================\n",
    "# 3) TI·ªÜN √çCH TO√ÅN H·ªåC & T√çNH G√ìC KH·ªöP\n",
    "# =====================================\n",
    "def unit_vector(v: np.ndarray) -> np.ndarray:\n",
    "    n = np.linalg.norm(v) + 1e-8\n",
    "    return v / n\n",
    "\n",
    "def angle_3pts(a: np.ndarray, b: np.ndarray, c: np.ndarray) -> float:\n",
    "    \"\"\"T√≠nh g√≥c ABC (ƒë·ªô) v·ªõi b l√† ƒë·ªânh, d·ª±a tr√™n 3 ƒëi·ªÉm 2D/3D.\"\"\"\n",
    "    ab = a - b\n",
    "    cb = c - b\n",
    "    uab, ucb = unit_vector(ab), unit_vector(cb)\n",
    "    cosang = np.clip(np.dot(uab, ucb), -1.0, 1.0)\n",
    "    return float(np.degrees(np.arccos(cosang)))\n",
    "\n",
    "# B·ªô g√≥c ti√™u bi·ªÉu t·ª´ Mediapipe (d√πng ID joints)\n",
    "# C√≥ th·ªÉ tu·ª≥ bi·∫øn b·ªï sung\n",
    "ANGLE_TRIPLETS = [\n",
    "    (11, 13, 15), # vai-tren tay-tren c·ªï tay tr√°i\n",
    "    (12, 14, 16), # vai-tren tay-tren c·ªï tay ph·∫£i\n",
    "    (23, 25, 27), # h√¥ng-g·ªëi-c·ªï ch√¢n tr√°i\n",
    "    (24, 26, 28), # h√¥ng-g·ªëi-c·ªï ch√¢n ph·∫£i\n",
    "    (11, 23, 25), # vai tr√°i - h√¥ng tr√°i - g·ªëi tr√°i\n",
    "    (12, 24, 26), # vai ph·∫£i - h√¥ng ph·∫£i - g·ªëi ph·∫£i\n",
    "]\n",
    "\n",
    "def normalize_landmarks(landmarks: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Chu·∫©n ho√° 33x(2 or 3) landmarks v·ªÅ h·ªá to·∫° ƒë·ªô t∆∞∆°ng ƒë·ªëi:\n",
    "    - T·ªãnh ti·∫øn theo h√¥ng trung b√¨nh (pelvis: mean(23,24))\n",
    "    - Chia t·ªâ l·ªá theo kho·∫£ng c√°ch vai (11,12)\n",
    "    \"\"\"\n",
    "    assert landmarks.ndim == 2\n",
    "    assert landmarks.shape[0] == MP_LANDMARK_NUM\n",
    "    pelvis = (landmarks[23] + landmarks[24]) / 2.0\n",
    "    shoulders = (landmarks[11], landmarks[12])\n",
    "    scale = np.linalg.norm(shoulders[0] - shoulders[1]) + 1e-8\n",
    "\n",
    "    lm = landmarks - pelvis\n",
    "    lm = lm / scale\n",
    "    return lm\n",
    "\n",
    "def landmarks_to_feature(landmarks: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Chuy·ªÉn landmarks (33x2) -> vector ƒë·∫∑c tr∆∞ng 1D:\n",
    "    - T·ªça ƒë·ªô (x,y) ƒë√£ chu·∫©n ho√° (66 chi·ªÅu)\n",
    "    - G√≥c kh·ªõp (len(ANGLE_TRIPLETS)) chi·ªÅu\n",
    "    \"\"\"\n",
    "    lm_norm = normalize_landmarks(landmarks[:, :2])\n",
    "    feats = lm_norm.flatten()  # 66\n",
    "    # G√≥c\n",
    "    angles = []\n",
    "    for (i,j,k) in ANGLE_TRIPLETS:\n",
    "        a, b, c = lm_norm[i], lm_norm[j], lm_norm[k]\n",
    "        angles.append(angle_3pts(a,b,c))\n",
    "    feats = np.concatenate([feats, np.array(angles, dtype=np.float32)])\n",
    "    return feats.astype(np.float32)\n",
    "\n",
    "print(\"Loaded math utils.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8f5ac0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H√†m tr√≠ch xu·∫•t ƒë√£ s·∫µn s√†ng. C·∫•u tr√∫c dataset: data_videos/<action>/*.mp4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =======================================================\n",
    "# 4) TR√çCH XU·∫§T KEYPOINTS T·ª™ VIDEO (ho·∫∑c webcam) -> .npz\n",
    "# =======================================================\n",
    "def extract_features_from_video(\n",
    "    video_path: str,\n",
    "    flip_horizontal: bool = True,\n",
    "    save_npz_path: Optional[str] = None,\n",
    "    use_world: bool = False,\n",
    "    max_frames: Optional[int] = None,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Tr√≠ch xu·∫•t 33 landmarks (2D) t·ª´ MediaPipe Pose, chuy·ªÉn th√†nh feature theo frame.\n",
    "    Tr·∫£ v·ªÅ dict: { 'features': (T,F), 'landmarks': (T,33,2), 'valid': (T,), 'fps': float }\n",
    "    \"\"\"\n",
    "    if mp is None:\n",
    "        raise RuntimeError(\"mediapipe ch∆∞a s·∫µn s√†ng. H√£y c√†i mediapipe tr∆∞·ªõc.\")\n",
    "    mp_pose = mp.solutions.pose\n",
    "\n",
    "    cap = cv2.VideoCapture(0 if video_path == 'webcam' else video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(f\"Kh√¥ng m·ªü ƒë∆∞·ª£c ngu·ªìn video: {video_path}\")\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "    frames_feats, frames_lm, valid = [], [], []\n",
    "\n",
    "    with mp_pose.Pose(static_image_mode=False, model_complexity=1,\n",
    "                      enable_segmentation=False, smooth_landmarks=True) as pose:\n",
    "        t = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if flip_horizontal:\n",
    "                frame = cv2.flip(frame, 1)\n",
    "            h, w = frame.shape[:2]\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            res = pose.process(rgb)\n",
    "\n",
    "            if res.pose_landmarks:\n",
    "                lm = np.array([(p.x*w, p.y*h) for p in res.pose_landmarks.landmark], dtype=np.float32)\n",
    "                feat = landmarks_to_feature(lm)\n",
    "                frames_feats.append(feat)\n",
    "                frames_lm.append(lm)\n",
    "                valid.append(1)\n",
    "            else:\n",
    "                # khung kh√¥ng nh·∫≠n landmark\n",
    "                if len(frames_feats) > 0:\n",
    "                    frames_feats.append(frames_feats[-1])  # gi·ªØ last-known\n",
    "                    frames_lm.append(frames_lm[-1])\n",
    "                else:\n",
    "                    frames_feats.append(np.zeros(66+len(ANGLE_TRIPLETS), dtype=np.float32))\n",
    "                    frames_lm.append(np.zeros((MP_LANDMARK_NUM,2), dtype=np.float32))\n",
    "                valid.append(0)\n",
    "\n",
    "            t += 1\n",
    "            if (max_frames is not None) and (t >= max_frames):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    feats = np.stack(frames_feats, axis=0) if frames_feats else np.zeros((0, 66+len(ANGLE_TRIPLETS)), dtype=np.float32)\n",
    "    lms = np.stack(frames_lm, axis=0) if frames_lm else np.zeros((0, MP_LANDMARK_NUM, 2), dtype=np.float32)\n",
    "    valid = np.array(valid, dtype=np.uint8)\n",
    "    out = {'features': feats, 'landmarks': lms, 'valid': valid, 'fps': float(fps)}\n",
    "\n",
    "    if save_npz_path is not None:\n",
    "        np.savez_compressed(save_npz_path, **out)\n",
    "        print(\"ƒê√£ l∆∞u:\", save_npz_path)\n",
    "    return out\n",
    "\n",
    "print(\"H√†m tr√≠ch xu·∫•t ƒë√£ s·∫µn s√†ng. C·∫•u tr√∫c dataset: data_videos/<action>/*.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "747314cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H√†m g·ªôp d·ªØ li·ªáu train s·∫µn s√†ng.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================\n",
    "# 5) TI·ªÄN X·ª¨ L√ù & G·ªòP D·ªÆ LI·ªÜU TRAIN THEO ACTION\n",
    "# ===================================================\n",
    "def build_training_matrix(action: str) -> Tuple[np.ndarray, List[int], List[float]]:\n",
    "    \"\"\"ƒê·ªçc to√†n b·ªô .mp4 / .mov trong data_videos/<action>,\n",
    "    tr√≠ch xu·∫•t features v√† g·ªôp th√†nh ma tr·∫≠n (T,F). Tr·∫£ v·ªÅ:\n",
    "      X_all: (N,F) ‚Äì n·ªëi c√°c frames\n",
    "      seg_ids: danh s√°ch cumulative s·ªë frame theo video ƒë·ªÉ c√≥ th·ªÉ quy chi·∫øu\n",
    "      fps_list: fps t·ª´ng video\n",
    "    \"\"\"\n",
    "    act_dir = DATA_DIR / action\n",
    "    act_dir.mkdir(parents=True, exist_ok=True)\n",
    "    video_paths = [p for p in act_dir.glob(\"*.*\") if p.suffix.lower() in [\".mp4\",\".mov\",\".avi\",\".mkv\"]]\n",
    "    X_all = []\n",
    "    seg_ids = []\n",
    "    fps_list = []\n",
    "    for vid in video_paths:\n",
    "        npz_path = FEAT_DIR / f\"{action}__{vid.stem}.npz\"\n",
    "        if not npz_path.exists():\n",
    "            _ = extract_features_from_video(str(vid), flip_horizontal=True, save_npz_path=str(npz_path))\n",
    "        data = np.load(npz_path, allow_pickle=True)\n",
    "        feats = data[\"features\"]\n",
    "        X_all.append(feats)\n",
    "        seg_ids.append(len(feats))\n",
    "        fps_list.append(float(data[\"fps\"]))\n",
    "    if len(X_all) == 0:\n",
    "        raise RuntimeError(f\"Kh√¥ng c√≥ video cho action '{action}' trong {act_dir}. H√£y th√™m video r·ªìi ch·∫°y l·∫°i.\")\n",
    "    X_all = np.concatenate(X_all, axis=0)\n",
    "    return X_all, seg_ids, fps_list\n",
    "\n",
    "print(\"H√†m g·ªôp d·ªØ li·ªáu train s·∫µn s√†ng.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f767dd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H√†m hu·∫•n luy·ªán pha ƒë√£ s·∫µn s√†ng.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================\n",
    "# 6) HU·∫§N LUY·ªÜN PHA A‚ÜíE CHO M·ªñI ACTION\n",
    "# ===================================================\n",
    "def train_action_phases(\n",
    "    action: str,\n",
    "    num_phases: int = NUM_PHASES,\n",
    "    method: str = \"auto\"  # 'auto'|'hmm'|'kmeans'\n",
    ") -> Dict:\n",
    "    \"\"\"H·ªçc pha A‚ÜíE t·ª´ d·ªØ li·ªáu frame-level:\n",
    "    - N·∫øu c√≥ hmmlearn: d√πng HMM (GaussianHMM) -> states ~ phases\n",
    "    - N·∫øu kh√¥ng: KMeans tr√™n to√†n b·ªô frames, sau ƒë√≥ s·∫Øp x·∫øp cluster theo th·ªùi gian xu·∫•t hi·ªán\n",
    "    Tr·∫£ v·ªÅ dict template g·ªìm:\n",
    "      {\n",
    "        'action': action,\n",
    "        'num_phases': num_phases,\n",
    "        'scaler': scaler (mean/std),\n",
    "        'centroids': [centroid_feat_i],\n",
    "        'tolerances': [tol_i]  # MAD-based\n",
    "      }\n",
    "    \"\"\"\n",
    "    X_all, seg_ids, fps_list = build_training_matrix(action)\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X_all)\n",
    "\n",
    "    # T√≠nh ch·ªâ s·ªë th·ªùi gian xu·∫•t hi·ªán ∆∞u th·∫ø ƒë·ªÉ s·∫Øp th·ª© t·ª± pha\n",
    "    frame_indices = np.arange(len(Xs))\n",
    "\n",
    "    if method == \"auto\":\n",
    "        method = \"hmm\" if HAVE_HMM else \"kmeans\"\n",
    "\n",
    "    if method == \"hmm\" and HAVE_HMM:\n",
    "        # HMM Gaussian: states ~ phases\n",
    "        hmm = GaussianHMM(n_components=num_phases, covariance_type='full', n_iter=100, verbose=False, random_state=42)\n",
    "        hmm.fit(Xs)\n",
    "        states = hmm.predict(Xs)\n",
    "        # Th·ª© t·ª± pha = th·ª© t·ª± trung v·ªã ch·ªâ s·ªë frame c·ªßa t·ª´ng state\n",
    "        order = []\n",
    "        for s in range(num_phases):\n",
    "            idx = np.where(states == s)[0]\n",
    "            median_t = np.median(idx) if len(idx) else 1e9\n",
    "            order.append((median_t, s))\n",
    "        order.sort()\n",
    "        ordered_states = [s for _, s in order]\n",
    "        # T√≠nh centroid theo state ƒë√£ s·∫Øp\n",
    "        centroids = []\n",
    "        tolerances = []\n",
    "        for s in ordered_states:\n",
    "            idx = np.where(states == s)[0]\n",
    "            if len(idx) == 0:\n",
    "                centroids.append(np.zeros(Xs.shape[1], dtype=np.float32))\n",
    "                tolerances.append(DEFAULT_TOL)\n",
    "                continue\n",
    "            c = Xs[idx].mean(axis=0)\n",
    "            centroids.append(c.astype(np.float32))\n",
    "            # MAD-based tol\n",
    "            dist = np.linalg.norm(Xs[idx] - c, axis=1)\n",
    "            mad = np.median(np.abs(dist - np.median(dist))) + 1e-6\n",
    "            tolerances.append(float(2.5 * mad))  # h·ªá s·ªë c√≥ th·ªÉ tinh ch·ªânh\n",
    "    else:\n",
    "        # KMeans fallback\n",
    "        km = KMeans(n_clusters=num_phases, random_state=42, n_init=10)\n",
    "        states = km.fit_predict(Xs)\n",
    "        # S·∫Øp x·∫øp cluster theo th·ªùi ƒëi·ªÉm xu·∫•t hi·ªán trung v·ªã\n",
    "        order = []\n",
    "        for s in range(num_phases):\n",
    "            idx = np.where(states == s)[0]\n",
    "            median_t = np.median(idx) if len(idx) else 1e9\n",
    "            order.append((median_t, s))\n",
    "        order.sort()\n",
    "        ordered_states = [s for _, s in order]\n",
    "        # T√≠nh centroid theo cluster ƒë√£ s·∫Øp\n",
    "        centroids = []\n",
    "        tolerances = []\n",
    "        for s in ordered_states:\n",
    "            idx = np.where(states == s)[0]\n",
    "            if len(idx) == 0:\n",
    "                centroids.append(np.zeros(Xs.shape[1], dtype=np.float32))\n",
    "                tolerances.append(DEFAULT_TOL)\n",
    "                continue\n",
    "            c = Xs[idx].mean(axis=0)\n",
    "            centroids.append(c.astype(np.float32))\n",
    "            dist = np.linalg.norm(Xs[idx] - c, axis=1)\n",
    "            mad = np.median(np.abs(dist - np.median(dist))) + 1e-6\n",
    "            tolerances.append(float(2.5 * mad))\n",
    "\n",
    "    template = {\n",
    "        \"action\": action,\n",
    "        \"num_phases\": num_phases,\n",
    "        \"scaler_mean\": scaler.mean_.tolist(),\n",
    "        \"scaler_scale\": scaler.scale_.tolist(),\n",
    "        \"centroids\": [c.tolist() for c in centroids],\n",
    "        \"tolerances\": tolerances,\n",
    "        \"feature_dim\": int(Xs.shape[1]),\n",
    "        \"angle_triplets\": ANGLE_TRIPLETS,\n",
    "        \"created_at\": time.time()\n",
    "    }\n",
    "    out_path = MODEL_DIR / f\"{action}_template.json\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(template, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u template A‚ÜíE cho action='{action}' v√†o {out_path}\")\n",
    "    print(\"Pha t∆∞∆°ng ·ª©ng: A=0, B=1, C=2, D=3, E=4\")\n",
    "    return template\n",
    "\n",
    "print(\"H√†m hu·∫•n luy·ªán pha ƒë√£ s·∫µn s√†ng.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff96ad31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H√†m ƒë·ªçc template & hi·ªáu ch·ªânh ƒë√£ s·∫µn s√†ng.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================\n",
    "# 7) ƒê·ªåC TEMPLATE & HI·ªÜU CH·ªàNH (CALIBRATION)\n",
    "# ===================================================\n",
    "def load_action_template(action: str) -> Dict:\n",
    "    path = MODEL_DIR / f\"{action}_template.json\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Ch∆∞a c√≥ template cho action '{action}'. H√£y train tr∆∞·ªõc (Cell 6).\")\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        tpl = json.load(f)\n",
    "    return tpl\n",
    "\n",
    "def make_scaler_from_template(tpl: Dict):\n",
    "    mean = np.array(tpl[\"scaler_mean\"], dtype=np.float32)\n",
    "    scale = np.array(tpl[\"scaler_scale\"], dtype=np.float32)\n",
    "    def transform(X):\n",
    "        return (X - mean) / (scale + 1e-8)\n",
    "    return transform\n",
    "\n",
    "def calibrate_user_from_frames(frames_feats: np.ndarray, tpl: Dict) -> Dict:\n",
    "    \"\"\"Hi·ªáu ch·ªânh dung sai theo ng∆∞·ªùi d√πng: ƒëo kho·∫£ng c√°ch ƒë·∫øn t·ª´ng centroid\n",
    "    trong m·ªôt ƒëo·∫°n kh·ªüi ƒë·ªông ng·∫Øn -> ƒëi·ªÅu ch·ªânh tolerance (n·ªõi/si·∫øt) h·ª£p l√Ω.\n",
    "    \"\"\"\n",
    "    transform = make_scaler_from_template(tpl)\n",
    "    Xs = transform(frames_feats)\n",
    "    centroids = [np.array(c, dtype=np.float32) for c in tpl[\"centroids\"]]\n",
    "    # T√≠nh kho·∫£ng c√°ch t·ªëi thi·ªÉu ƒë·∫øn m·ªói centroid\n",
    "    dists = []\n",
    "    for i, c in enumerate(centroids):\n",
    "        dist_i = np.linalg.norm(Xs - c[None,:], axis=1)\n",
    "        dists.append(np.median(dist_i))\n",
    "    # T·∫°o scale factor d·ª±a tr√™n median kho·∫£ng c√°ch\n",
    "    med = np.median(dists)\n",
    "    cal_factor = float(np.clip(med, 0.5, 2.0))  # gi·ªõi h·∫°n\n",
    "    adj_tols = [float(t * cal_factor) for t in tpl[\"tolerances\"]]\n",
    "    return {\n",
    "        \"adj_tolerances\": adj_tols,\n",
    "        \"cal_factor\": cal_factor\n",
    "    }\n",
    "\n",
    "print(\"H√†m ƒë·ªçc template & hi·ªáu ch·ªânh ƒë√£ s·∫µn s√†ng.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31af89de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H√†m ƒë√°nh gi√° khung x∆∞∆°ng ƒë√£ s·∫µn s√†ng.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================\n",
    "# 8) ƒê√ÅNH GI√Å KHUNG X∆Ø∆†NG ƒê√öNG/SAI THEO PHA\n",
    "# ===================================================\n",
    "def phase_similarity(feat: np.ndarray, tpl: Dict, phase_idx: int) -> float:\n",
    "    transform = make_scaler_from_template(tpl)\n",
    "    X = transform(feat)\n",
    "    c = np.array(tpl[\"centroids\"][phase_idx], dtype=np.float32)\n",
    "    return float(np.linalg.norm(X - c))\n",
    "\n",
    "def joints_correct_mask(\n",
    "    lm: np.ndarray,\n",
    "    tpl_lm_ref: Optional[np.ndarray],\n",
    "    tol: float = 0.12\n",
    ") -> np.ndarray:\n",
    "    \"\"\"So s√°nh t·ª´ng kh·ªõp v·ªõi template (d·∫°ng landmark 2D ƒë√£ chu·∫©n ho√°),\n",
    "    tr·∫£ v·ªÅ mask ƒë√∫ng/sai cho 33 joints.\n",
    "    \"\"\"\n",
    "    if tpl_lm_ref is None:\n",
    "        return np.ones((MP_LANDMARK_NUM,), dtype=np.uint8)\n",
    "    lm_norm = normalize_landmarks(lm)\n",
    "    d = np.linalg.norm(lm_norm - tpl_lm_ref, axis=1)  # per-joint\n",
    "    return (d < tol).astype(np.uint8)\n",
    "\n",
    "def centroid_to_landmarks(centroid_feat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Chuy·ªÉn m·ªôt centroid feature (66 + angles) v·ªÅ landmarks 33x2 (chu·∫©n ho√°).\"\"\"\n",
    "    coords = centroid_feat[:66].reshape(MP_LANDMARK_NUM, 2)\n",
    "    return coords\n",
    "\n",
    "print(\"H√†m ƒë√°nh gi√° khung x∆∞∆°ng ƒë√£ s·∫µn s√†ng.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dedb241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================================================\n",
    "# 9) REAL-TIME SHADOWING UI (ABC... + SKELETON)\n",
    "# ===================================================\n",
    "class ShadowingUI:\n",
    "    def __init__(self, window_name=\"Shadowing\", scale=1.0, fullscreen=False):\n",
    "        self.window = window_name\n",
    "        self.scale = scale\n",
    "        self.fullscreen = fullscreen\n",
    "        self.click_points = []\n",
    "        cv2.namedWindow(self.window, cv2.WINDOW_NORMAL)\n",
    "        if self.fullscreen:\n",
    "            cv2.setWindowProperty(self.window, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "        cv2.setMouseCallback(self.window, self.on_mouse)\n",
    "\n",
    "    def on_mouse(self, event, x, y, flags, param):\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            self.click_points.append((x,y))\n",
    "\n",
    "    def draw_skeleton(self, frame, lm, correct_mask):\n",
    "        for (i,j) in SKELETON_EDGES:\n",
    "            p1 = tuple(lm[i].astype(int))\n",
    "            p2 = tuple(lm[j].astype(int))\n",
    "            ok = bool(correct_mask[i] and correct_mask[j])\n",
    "            color = (0,255,0) if ok else (0,0,255)\n",
    "            cv2.line(frame, p1, p2, color, 2)\n",
    "        # joints\n",
    "        for idx, p in enumerate(lm):\n",
    "            ok = bool(correct_mask[idx])\n",
    "            color = (0,255,0) if ok else (0,0,255)\n",
    "            cv2.circle(frame, tuple(p.astype(int)), 3, color, -1)\n",
    "\n",
    "    def draw_phase_points(self, frame, phase_points: List[Tuple[int,int]], active_idx: int):\n",
    "        # V·∫Ω c√°c ƒëi·ªÉm ABCDE to, d·ªÖ b·∫•m\n",
    "        for k, (x,y) in enumerate(phase_points):\n",
    "            color = (0,200,255) if k == active_idx else (200,200,200)\n",
    "            cv2.circle(frame, (x,y), 18, color, -1)\n",
    "            cv2.putText(frame, chr(ord('A')+k), (x-7, y+6), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (20,20,20), 2)\n",
    "\n",
    "    def present(self, frame):\n",
    "        if self.scale != 1.0:\n",
    "            h,w = frame.shape[:2]\n",
    "            frame = cv2.resize(frame, (int(w*self.scale), int(h*self.scale)))\n",
    "        cv2.imshow(self.window, frame)\n",
    "\n",
    "    def handle_keys(self) -> bool:\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            return False\n",
    "        elif key == ord('+') or key == ord('='):\n",
    "            self.scale = min(3.0, self.scale + 0.1)\n",
    "        elif key == ord('-') or key == ord('_'):\n",
    "            self.scale = max(0.5, self.scale - 0.1)\n",
    "        elif key == ord('f'):\n",
    "            self.fullscreen = not self.fullscreen\n",
    "            cv2.setWindowProperty(self.window, cv2.WND_PROP_FULLSCREEN,\n",
    "                                  cv2.WINDOW_FULLSCREEN if self.fullscreen else cv2.WINDOW_NORMAL)\n",
    "        return True\n",
    "\n",
    "def run_shadowing(action: str = \"serve\", flip_horizontal: bool = True):\n",
    "    if mp is None:\n",
    "        raise RuntimeError(\"mediapipe ch∆∞a ƒë∆∞·ª£c c√†i. Kh√¥ng th·ªÉ ch·∫°y real-time.\")\n",
    "    tpl = load_action_template(action)\n",
    "    transform = make_scaler_from_template(tpl)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Kh√¥ng m·ªü ƒë∆∞·ª£c webcam.\")\n",
    "    ui = ShadowingUI(window_name=f\"Shadowing - {action}\", scale=1.2, fullscreen=False)\n",
    "    mp_pose = mp.solutions.pose\n",
    "\n",
    "    curr_phase = 0  # A=0,...,E=4\n",
    "    achieved = [False]*tpl[\"num_phases\"]\n",
    "    start_time, end_time = None, None\n",
    "    phase_points = None  # to·∫° ƒë·ªô l·ªõn ƒë·ªÉ ng∆∞·ªùi d√πng b·∫•m theo tr√¨nh t·ª±\n",
    "\n",
    "    with mp_pose.Pose(static_image_mode=False, model_complexity=1,\n",
    "                      enable_segmentation=False, smooth_landmarks=True) as pose:\n",
    "        t0 = time.time()\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if flip_horizontal:\n",
    "                frame = cv2.flip(frame, 1)\n",
    "            h,w = frame.shape[:2]\n",
    "\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            res = pose.process(rgb)\n",
    "\n",
    "            if res.pose_landmarks:\n",
    "                lm = np.array([(p.x*w, p.y*h) for p in res.pose_landmarks.landmark], dtype=np.float32)\n",
    "                feat = landmarks_to_feature(lm)\n",
    "                # so kh·ªõp v·ªõi phase hi·ªán t·∫°i\n",
    "                dist = phase_similarity(feat, tpl, curr_phase)\n",
    "                tol = tpl[\"tolerances\"][curr_phase]\n",
    "                # khung x∆∞∆°ng ƒë√∫ng/sai per-joint\n",
    "                tpl_lm = centroid_to_landmarks(np.array(tpl[\"centroids\"][curr_phase], dtype=np.float32))\n",
    "                mask = joints_correct_mask(lm, tpl_lm, tol=0.12)\n",
    "                # v·∫Ω skeleton\n",
    "                ui.draw_skeleton(frame, lm, mask)\n",
    "\n",
    "                # T·∫°o ƒëi·ªÉm ABCDE l·ªõn tr√™n m√†n h√¨nh (g·∫ßn v·ªã tr√≠ kh·ªõp quan tr·ªçng ‚Äì v√≠ d·ª• c·ªï tay ph·∫£i theo template)\n",
    "                if phase_points is None:\n",
    "                    # L·∫•y 1 kh·ªõp ƒë·∫°i di·ªán (vd: c·ªï tay ph·∫£i 16, khu·ª∑u 14, vai 12) t·ª´ template v√† chi·∫øu t·ªâ l·ªá l√™n khung h√¨nh\n",
    "                    tpl_lm_norm = tpl_lm  # ƒë√£ chu·∫©n ho√°\n",
    "                    # Chi·∫øu ƒë∆°n gi·∫£n: l·∫•y vai ph·∫£i (12) l√†m g·ªëc g·∫ßn gi·ªØa ·∫£nh\n",
    "                    base = np.array([w*0.7, h*0.3])\n",
    "                    # ƒê·∫∑t 5 ƒëi·ªÉm theo c·ªôt d·ªçc\n",
    "                    phase_points = [(int(base[0]), int(base[1] + i*60)) for i in range(tpl[\"num_phases\"])]\n",
    "\n",
    "                # Ki·ªÉm tra click v√†o ƒë√∫ng ƒëi·ªÉm k·∫ø ti·∫øp\n",
    "                if ui.click_points:\n",
    "                    cx, cy = ui.click_points.pop(0)\n",
    "                    px, py = phase_points[curr_phase]\n",
    "                    if (cx - px)**2 + (cy - py)**2 < 28**2:\n",
    "                        # click tr√∫ng ƒëi·ªÉm c·ªßa phase hi·ªán t·∫°i -> x√°c nh·∫≠n\n",
    "                        achieved[curr_phase] = True\n",
    "\n",
    "                # N·∫øu kho·∫£ng c√°ch < tol -> coi nh∆∞ ƒë·∫°t pha\n",
    "                if dist < tol:\n",
    "                    achieved[curr_phase] = True\n",
    "\n",
    "                # Khi ƒë·∫°t A l·∫ßn ƒë·∫ßu -> ƒë√°nh d·∫•u start_time\n",
    "                if achieved[0] and start_time is None:\n",
    "                    start_time = time.time()\n",
    "\n",
    "                # N·∫øu ƒë√£ ƒë·∫°t phase hi·ªán t·∫°i -> chuy·ªÉn ti·∫øp\n",
    "                if achieved[curr_phase] and curr_phase < tpl[\"num_phases\"] - 1:\n",
    "                    curr_phase += 1\n",
    "                elif achieved[-1]:\n",
    "                    end_time = time.time()\n",
    "                    cv2.putText(frame, \"Sequence COMPLETED!\", (30, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,255,0), 2)\n",
    "\n",
    "                # V·∫Ω ƒëi·ªÉm ABCDE\n",
    "                ui.draw_phase_points(frame, phase_points, curr_phase)\n",
    "\n",
    "                # Overlay th√¥ng tin\n",
    "                cv2.putText(frame, f\"Phase: {chr(ord('A')+curr_phase)}  dist={dist:.3f} tol={tol:.3f}\",\n",
    "                            (20, h-20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)\n",
    "            else:\n",
    "                cv2.putText(frame, \"No pose detected\", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,0,255), 2)\n",
    "\n",
    "            ui.present(frame)\n",
    "            if not ui.handle_keys():\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    if start_time and end_time:\n",
    "        result = {\n",
    "            \"action\": action,\n",
    "            \"start_time_unix\": start_time,\n",
    "            \"end_time_unix\": end_time,\n",
    "            \"duration_sec\": end_time - start_time\n",
    "        }\n",
    "        out_json = MODEL_DIR / f\"last_{action}_segment.json\"\n",
    "        with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "        print(\"ƒê√£ l∆∞u th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu/k·∫øt th√∫c:\", out_json)\n",
    "    else:\n",
    "        print(\"Ch∆∞a ho√†n t·∫•t tr·ªçn v·∫πn chu·ªói A‚ÜíE.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f0110a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H√†m ph√¢n ƒëo·∫°n video ƒë√£ s·∫µn s√†ng.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================\n",
    "# 10) PH√ÇN ƒêO·∫†N VIDEO M·ªöI -> JSON (B·∫ÆT ƒê·∫¶U / K·∫æT TH√öC)\n",
    "# ===================================================\n",
    "def segment_video_by_template(video_path: str, action: str, flip_horizontal=True) -> Dict:\n",
    "    tpl = load_action_template(action)\n",
    "    data = extract_features_from_video(video_path, flip_horizontal=flip_horizontal, save_npz_path=None)\n",
    "    feats, valid, fps = data[\"features\"], data[\"valid\"], float(data[\"fps\"])\n",
    "    transform = make_scaler_from_template(tpl)\n",
    "\n",
    "    curr_phase = 0\n",
    "    in_sequence = False\n",
    "    start_idx, end_idx = None, None\n",
    "\n",
    "    for i in range(len(feats)):\n",
    "        f = feats[i]\n",
    "        dist = phase_similarity(f, tpl, curr_phase)\n",
    "        tol = tpl[\"tolerances\"][curr_phase]\n",
    "        if dist < tol:\n",
    "            if curr_phase == 0 and not in_sequence:\n",
    "                in_sequence = True\n",
    "                start_idx = i\n",
    "            if curr_phase < tpl[\"num_phases\"]-1:\n",
    "                curr_phase += 1\n",
    "            else:\n",
    "                end_idx = i\n",
    "                break\n",
    "\n",
    "    result = {\n",
    "        \"action\": action,\n",
    "        \"video\": video_path,\n",
    "        \"start_frame\": int(start_idx) if start_idx is not None else None,\n",
    "        \"end_frame\": int(end_idx) if end_idx is not None else None,\n",
    "        \"fps\": fps,\n",
    "        \"start_time_sec\": (start_idx / fps) if start_idx is not None else None,\n",
    "        \"end_time_sec\": (end_idx / fps) if end_idx is not None else None\n",
    "    }\n",
    "    out_json = MODEL_DIR / f\"segment_{Path(video_path).stem}_{action}.json\"\n",
    "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "    print(\"K·∫øt qu·∫£ ph√¢n ƒëo·∫°n l∆∞u t·∫°i:\", out_json)\n",
    "    return result\n",
    "\n",
    "print(\"H√†m ph√¢n ƒëo·∫°n video ƒë√£ s·∫µn s√†ng.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c52bbf",
   "metadata": {},
   "source": [
    "\n",
    "## üöÄ H∆∞·ªõng d·∫´n ch·∫°y nhanh\n",
    "\n",
    "1. **Chu·∫©n b·ªã d·ªØ li·ªáu hu·∫•n luy·ªán**  \n",
    "   - Th·∫£ video v√†o th∆∞ m·ª•c `data_videos/<action>/`. V√≠ d·ª•: `data_videos/serve/serve_01.mp4`.\n",
    "   - S·ª≠a danh s√°ch `ACTIONS` ·ªü **Cell 2** n·∫øu c√≥ th√™m ƒë·ªông t√°c.\n",
    "\n",
    "2. **Tr√≠ch xu·∫•t & Hu·∫•n luy·ªán**  \n",
    "   - Ch·∫°y **Cell 5** ƒë·ªÉ g·ªôp d·ªØ li·ªáu.  \n",
    "   - Ch·∫°y **Cell 6**: `train_action_phases(\"serve\")` ƒë·ªÉ t·∫°o template A‚ÜíE.\n",
    "\n",
    "3. **Hi·ªáu ch·ªânh dung sai theo ng∆∞·ªùi d√πng (tu·ª≥ ch·ªçn)**  \n",
    "   - Quay m·ªôt ƒëo·∫°n ng·∫Øn b·∫±ng webcam: ch·∫°y **Cell 4** v·ªõi `video_path=\"webcam\", max_frames=300` ƒë·ªÉ l·∫•y `frames_feats`.  \n",
    "   - G·ªçi **Cell 7**: `calibrate_user_from_frames(frames_feats, tpl)` ƒë·ªÉ ƒëi·ªÅu ch·ªânh `tolerances`.\n",
    "\n",
    "4. **Ch·∫°y real-time shadowing**  \n",
    "   - Ch·∫°y **Cell 9**: `run_shadowing(\"serve\")`.  \n",
    "   - Ph√≠m t·∫Øt: **q** tho√°t, **+/-** ph√≥ng to/thu nh·ªè, **f** to√†n m√†n h√¨nh.  \n",
    "   - **Click v√†o c√°c ƒëi·ªÉm A‚ÄìE** theo tr√¨nh t·ª± ho·∫∑c ƒë∆°n gi·∫£n l√† th·ª±c hi·ªán ƒë·ªông t√°c ƒë√∫ng ƒë·ªÉ t·ª± qua pha.  \n",
    "   - Khung x∆∞∆°ng: **xanh** = ƒë√∫ng, **ƒë·ªè** = sai.\n",
    "\n",
    "5. **Ph√¢n ƒëo·∫°n video offline**  \n",
    "   - D√πng **Cell 10**: `segment_video_by_template(\"test.mp4\", \"serve\")` ƒë·ªÉ nh·∫≠n `start_time_sec`/`end_time_sec` d·∫°ng JSON.\n",
    "\n",
    "> üß© B·∫°n c√≥ th·ªÉ thay **KMeans** b·∫±ng **HMM** (n·∫øu c√†i `hmmlearn`) ch·ªâ b·∫±ng tham s·ªë `method=\"hmm\"` trong `train_action_phases`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada7dddc",
   "metadata": {},
   "source": [
    "\n",
    "## üß† M·∫πo & M·ªü r·ªông\n",
    "- **ƒê·ªô ch√≠nh x√°c cao h∆°n**: tƒÉng ch·∫•t l∆∞·ª£ng video (√°nh s√°ng), d√πng `model_complexity=2` c·ªßa MediaPipe, tƒÉng s·ªë pha >5 ƒë·ªëi v·ªõi ƒë·ªông t√°c ph·ª©c t·∫°p.\n",
    "- **T√¥ m√†u theo kh·ªõp quan tr·ªçng**: c√¢n n·∫∑ng ho√° c√°c joints (vd: vai‚Äìkhu·ª∑u‚Äìc·ªï tay khi *serve*).\n",
    "- **ƒêi·ªÉm A‚ÄìE theo h√¨nh h·ªçc th·ª±c**: hi·ªán t·∫°i demo ƒë·∫∑t A‚ÄìE ·ªü c·ªôt ph·∫£i. B·∫°n c√≥ th·ªÉ n·ªôi suy t·ª´ centroid template -> chi·∫øu l√™n ·∫£nh theo scale/offset ƒë·ªÉ s√°t t∆∞ th·∫ø h∆°n.\n",
    "- **L∆∞u v·∫øt luy·ªán t·∫≠p**: ghi log ƒë·ªô l·ªách t·ª´ng pha, s·ªë l·∫ßn th·ª≠, th·ªùi gian ho√†n th√†nh ƒë·ªÉ feedback ti·∫øn b·ªô.\n",
    "- **Thi·∫øt b·ªã c·∫£m ·ª©ng**: OpenCV h·ªó tr·ª£ **click chu·ªôt**; n·∫øu m√†n h√¨nh c·∫£m ·ª©ng map th√†nh s·ª± ki·ªán chu·ªôt, v·∫´n ho·∫°t ƒë·ªông.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
